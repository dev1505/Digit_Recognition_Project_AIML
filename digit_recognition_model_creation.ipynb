{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset and splitting it into training and testing data\n",
    "dataset = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the training and testing data\n",
    "print(\"Length of the training data: \", len(x_train))\n",
    "print(\"Length of the testing data: \", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the first 10 images in the dataset in a 2x5 grid\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(30):\n",
    "    plt.subplot(6, 5, i+1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.title(y_train[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data to make it easier for the model to learn\n",
    "x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "x_train = x_train.astype('float32') / 255\n",
    "\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the first image in the dataset\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 filters: The layer uses 32 filters, each of size 3x3. These filters are like small kernels that slide across the input image, extracting features like edges, corners, or basic shapes.\n",
    "\n",
    "# (3, 3) kernel size: This specifies the size of the filter (kernel) used for convolution. Here, a 3x3 kernel is used, meaning it extracts features from a 3x3 region of the input image.\n",
    "\n",
    "# 'relu' activation: This defines the activation function as ReLU (Rectified Linear Unit). ReLU introduces non-linearity into the network, allowing it to learn more complex patterns.\n",
    "\n",
    "# input_shape=(28, 28, 1): This specifies the shape of the input data. This network expects grayscale images of size 28x28 pixels with a single color channel (1).\n",
    "\n",
    "# Max pooling layer with a window size of 2x2. Pooling helps to:\n",
    "\n",
    "# Reduce dimensionality: By taking the maximum value from a 2x2 window, the layer reduces the size of the data, making the network more efficient to train.\n",
    "\n",
    "# Increase robustness: Pooling can make the network less sensitive to small shifts in the input image, improving its generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "# This line creates a sequential model using tf.keras.models.Sequential(). In a sequential model, layers are added one after another, forming a linear stack. This is a common approach for building CNNs.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test accuracy: \", test_accuracy*100, \"%\")\n",
    "print(\"Test loss: \", test_loss*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"digit_recognizer.joblib\"\n",
    "joblib.dump(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = joblib.load(\"digit_recognizer.joblib\")\n",
    "\n",
    "predictions = new_model.predict(x_test)\n",
    "\n",
    "# Showing the first 10 predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(np.argmax(predictions[i]))\n",
    "    plt.xlabel(y_test[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting other things using seaborn\n",
    "sns.countplot(y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import joblib\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"Hand_Written_Images/img_3_3.png\"\n",
    "model = joblib.load(\"digit_recognizer.joblib\")\n",
    "\n",
    "\n",
    "def detect_and_invert(image):\n",
    "\n",
    "    # Thresholding to create binary image. Basically, we are converting the image to black and white\n",
    "    _, binary = cv2.threshold(image, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Count non-zero pixels (white pixels)\n",
    "    white_pixel_count = cv2.countNonZero(binary)\n",
    "\n",
    "    # Determine if inversion is needed based on threshold\n",
    "    threshold_value = 0.1 * binary.size\n",
    "\n",
    "    # Adjust this threshold as needed. If more than 10% of the pixels are white, invert the image\n",
    "    if white_pixel_count > threshold_value:\n",
    "\n",
    "        # Image is inverted, so invert it\n",
    "        inverted_image = np.invert(np.array([image]))\n",
    "        return inverted_image\n",
    "    else:\n",
    "        # Image is not inverted, return original\n",
    "        return np.array([image])\n",
    "\n",
    "\n",
    "def digit_recognizer_function(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.convert(\"L\")\n",
    "    img = img.resize((28, 28))\n",
    "    img = np.array(img)\n",
    "    img = detect_and_invert(img)\n",
    "    prediction = model.predict(img)\n",
    "    print(f\"Predicted Number: {np.argmax(prediction)}\")\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()\n",
    "\n",
    "digit_recognizer_function(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# import joblib\n",
    "\n",
    "# dataset = tf.keras.datasets.mnist\n",
    "# (x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
    "\n",
    "# x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "# x_train = x_train.astype('float32') / 255\n",
    "\n",
    "# x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "# x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# y_train = tf.keras.utils.to_categorical(y_train)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# model_name = \"digit_recognizer_other.joblib\"\n",
    "# joblib.dump(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# from skimage.filters import threshold_otsu\n",
    "\n",
    "\n",
    "# def segment_digits(image):\n",
    "#     image = np.array(image)\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     _, binary = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "#     contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     digit_images = []\n",
    "#     for contour in contours:\n",
    "#         x, y, w, h = cv2.boundingRect(contour)\n",
    "#         digit = image[y : y + h, x : x + w]\n",
    "#         digit = cv2.resize(digit, (28, 28))\n",
    "#         digit = digit / 255.0\n",
    "#         digit_images.append(digit.reshape(28, 28, 1))\n",
    "#     return digit_images, contours\n",
    "\n",
    "\n",
    "# def detect_and_invert(image):\n",
    "\n",
    "#     if len(image.shape) > 2:\n",
    "#         # Convert color image to grayscale\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Thresholding to create binary image\n",
    "#     _, binary = cv2.threshold(image, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#     # Count non-zero pixels (white pixels)\n",
    "#     white_pixel_count = cv2.countNonZero(binary)\n",
    "\n",
    "#     # Determine if inversion is needed based on threshold\n",
    "#     threshold_value = 0.1 * binary.size  # Adjust this threshold as needed\n",
    "#     if white_pixel_count > threshold_value:\n",
    "#         # Image is inverted, so invert it\n",
    "#         inverted_image = np.invert(np.array([image]))\n",
    "#         return inverted_image\n",
    "#     else:\n",
    "#         # Image is not inverted, return original\n",
    "#         return np.array([image])\n",
    "\n",
    "\n",
    "# img = Image.open(\"Hand_Written_Images/img_3_3.png\")\n",
    "# segmented_digits = segment_digits(img)\n",
    "# for digit in segmented_digits:\n",
    "\n",
    "#     digit = cv2.resize(digit, (28, 28))\n",
    "#     digit = detect_and_invert(digit)\n",
    "#     prediction = new_model.predict(digit)\n",
    "#     plt.imshow(digit.reshape((28, 28)), cmap=plt.cm.binary)\n",
    "#     plt.title(f\"Predicted value of Digit: {np.argmax(prediction)}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load(\"digit_recognizer.joblib\")\n",
    "\n",
    "# def preprocess_image(image_path):\n",
    "#     image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     _, thresh = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "#     return thresh\n",
    "\n",
    "\n",
    "# def segment_digits(thresh_image):\n",
    "#     contours, _ = cv2.findContours(\n",
    "#         thresh_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "#     )\n",
    "#     digit_images = []\n",
    "#     for contour in contours:\n",
    "#         x, y, w, h = cv2.boundingRect(contour)\n",
    "#         digit = thresh_image[y : y + h, x : x + w]\n",
    "#         digit = cv2.resize(digit, (28, 28))\n",
    "#         digit = digit / 255.0\n",
    "#         digit_images.append(digit.reshape(28, 28, 1))\n",
    "#     return digit_images, contours\n",
    "\n",
    "\n",
    "# def recognize_digits(digit_images):\n",
    "#     predictions = []\n",
    "#     for digit_image in digit_images:\n",
    "#         # digit_image = np.expand_dims(digit_image, axis=0)\n",
    "#         prediction = np.argmax(model.predict(digit_image))\n",
    "#         predictions.append(prediction)\n",
    "#         plt.imshow(digit_image.reshape((28, 28)), cmap=plt.cm.binary)\n",
    "#         plt.show()\n",
    "#     return predictions\n",
    "\n",
    "\n",
    "# def main(image_path):\n",
    "#     thresh_image = preprocess_image(image_path)\n",
    "#     digit_images, contours = segment_digits(thresh_image)\n",
    "#     # Extract x-coordinate of the bounding rectangle for each contour\n",
    "#     digit_images = sorted(\n",
    "#         zip(digit_images, contours), key=lambda x: cv2.boundingRect(x[1])[0]\n",
    "#     )\n",
    "#     digit_images = [digit[0] for digit in digit_images]  # Keep only the digit images\n",
    "#     predictions = recognize_digits(digit_images)\n",
    "#     phone_number = \"\".join(map(str, predictions))\n",
    "#     return phone_number\n",
    "\n",
    "\n",
    "# # Usage\n",
    "# image_path = \"Hand_Written_Images/img_1-2_2.png\"\n",
    "# phone_number = main(image_path)\n",
    "# print(\"Recognized Phone Number:\", phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# for img in os.listdir(\"Hand_Written_Images\"):\n",
    "#     img1 = img\n",
    "#     img = cv2.imread(f\"Hand_Written_Images/{img}\")\n",
    "#     img = cv2.resize(img, (28, 28))\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img = cv2.adaptiveThreshold(\n",
    "#         img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "#     )\n",
    "#     img = cv2.bitwise_not(img)\n",
    "#     img = img.reshape(1, 28, 28)\n",
    "#     # img = np.invert(img)\n",
    "#     prediction = new_model.predict(img)\n",
    "#     # displaying the prediction\n",
    "#     plt.title(f\"Predicted value of Digit: {np.argmax(prediction)}\")\n",
    "#     # displaying actual value of the digit\n",
    "#     plt.xlabel(f'Actual value of Digit: {img1.split(\"_\")[1]}_{img1.split(\"_\")[2]}')\n",
    "#     # displaying the image\n",
    "#     plt.imshow(img.reshape(28, 28), cmap=plt.cm.binary)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.open('Hand_Written_Images/img_7_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
